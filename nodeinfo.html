<!DOCTYPE html>

<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <title>Colour Clustering</title>
  <LINK href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BmbxuPwQa2lc/FVzBcNJ7UAyJxM6wuqIj61tLrc4wSX0szH/Ev+nYRRuWlolflfl" crossorigin="anonymous" />
  <LINK rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.3.0/font/bootstrap-icons.css" />
  <LINK href="styles.css" rel="stylesheet" type="text/css">

</head>
  <body id="home">
<img src="images/wideaesthetics.jpg" alt="wideaesthetics"  width=100% margin=0/>
 	
    <nav class="navbar navbar-expand-lg navbar-dark bg-secondary shadow-sm fixed-top">
      <div class="container">
        <a class="navbar-brand" href="#">Kokostino</a>
        <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarNav">
          <ul class="navbar-nav ms-auto">
            <li class="nav-item">
              <a class="nav-link" href="index.html">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="index.html#about">About</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="index.html#projects">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="index.html#contact">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

	<h2>Extracting colour and content information</h2>
<div class="container small_container" >
<p>
The goal of this project is to automatically create aesthetically looking sequences or groups of images.
Having hundreds or thousands of photos, this is a tedious task when done by hand.
In this project I set up a machine learning pipeline to do the work for me.
Two of the essential ingredients for a pleasing combination of photos are content and colour.
We use the information on those to find sets of images which combined are candy to the eye.</br></br>
The three major steps in the set up are:
<ol>
<li> Get the content of an image via multiclass image classification.</li></br>
<li> Get the dominating colours via clustering.</li></br>
<li> Use these inputs to generate an appealing sequence of images.</li></br>
</ol>
<img src="images/pipe.png" alt="pipe"  margin=0 width=100%/></br></br>

<b>Remark:</b> In addition to colour and content, also complexity and attention are important factors for aesthetics. The influence of those will be investigated in an upcoming project.

</p>


<h3>1. Extract the dominant colours with K-Medoids</h3>
<p>
The relevant information we want to extract here are the k dominant colours.
This is done by clustering the pixels into k clusters as shown below for k=5.</br></br>
<img src="images/KMe.png" alt="KMedoids"  width=50% margin=0 class="center"/></br></br>

K-Medoids is a clustering algorithm similar to K-Means, the difference being the cluster centers (centroids) 
not being at the geometric mean of the cluster but instead being the most central cluster data point itsself.
Why do we not just use K-Means, which is the more common and performant algorithm?
The answer has to do with how humans perceive similar colours vs how common colour spaces calculate similar colours.
The most common colour spaces, e.g. RGB or BGR, are three-dimensional Euclidean spaces.
K-Means for Euclidean spaces (or actually Euclidean metrics) works perfectly fine. 
It turns out though, the standard Euclidean colour space do not match human perception of colour distances.
For this we have to switch to CIELAB space, a three dimensional space with colour coordinates a and b and lighting coordinate L.
One of the improvements of CIELAB and its non-Euclidian metric compared to RGB is an improved illustration of lighting.
It took decades to come up with a metric which is uniform, i.e. distances in that space are uniform compared to human perception of colour distance.
The first version "CIE76" from 1976 was an Euclidean metric but lacked uniformity. 
The metric "CIEDE 2000" improved a lot in regards of perceptual uniformity. It is non-Euclidean
and computations are slowed down quite a bit. The following formula shows the metric:</br></br>

<img src="images/ciede.PNG"  width=35% class="center"/></br></br>
with &#916;L' corresponding to the lighting L and &#916;C' and &#916;H' depending non-trivially on the coordinates a and b.
The other factors are constants.
Distances in this metric are computationally expensive. Furthermore KMeans requires the computation of the geometric mean, 
hence we have to switch to another clustering algorithm: KMedoids.
The following cluster on the left which was computed uses K-Means illustrates the weakness of KMeans and RGB-space:</br></br>

<img src="images/cluster1.PNG"  width=50% margin-right=30 style="float:left"></br></br>
While the first two images are not completely far off, the last image is not matched well.
When we do the same computation with K-Medoids (top) in non-Euclidean CIELAB space we see that the dominant colours are extracted more realistically compared to K-Means (bottom).
</br></br></br>
<img src="images/MedvsMean.PNG" width=35%/>
</br></br></br>
</p>

</br></br></br></br>
<h3>2. Classify the content</h3>
<p>The goal of the image classification is to gain information on the content of the image to control in
which group of posts it appears. 
For instance a picture of a tropical beach next to a snowy mountain hut seems clashing, even if the colour scheme was harmonic.
Considering the fact that most of my pictures are travel and outdoor pics,
the image classification model will fit on the following classes: palmtrees, snow and neither of those.
This is a first order approximation but sufficient for now.
The data are cleanly separable as well, palmeras en la nieve have not been in the spotlight of my camera yet.
</p>
        <h5>2.1 Image Data Collection</h5><p>
		Taking my set of favourite pictures, only around 10% contain palmtrees and 5% fit the content "winter paradise".
		To avoid imbalanced training data, I scrape and clean the top palmtree images
		from my favourite photo sharing social media platform.
		Finding images which fit my definition of winter paradise was a bit less trivial and required more cleaning afterwards.
		The winter pictures I usually take are mountain or forest photos. 
		Hence I scraped images with the hashtags snowboarding, backcountriskiing, winterwald and snowshoeing.
		The final dataset consisted of roughly 2000 images, 25% of which contained palmtrees, 25% in the winter wonderland
		category and the remaining 50% fell into neither class.</br>
		The distributions of scraped and self-made photos are not fully identical but
		this is neglected in training and testing. After all, top posts with different style also influence the style of my future images.
</p>
<h5>2.2 Data Pre-processing</h5><p>
		To have the algorithm focus more on the essential part, some images are cropped such that the 
		desired object (usually the palmtree) is dominant in the image.
		Furthermore it turned out that a non-negligible part of scraped palmtree images contained white frames. 
		Considering that the competing category was snow-dominated, those had to be cropped off.
	</p>

<h5>2.3 Choose the best Model</h5>
<p> 
A set of 2000 images with quite non-uniform content is too small to reach a great accuracy on a fresh model.
The achieved accuracy on the validation set of a model with three convolutional layers was around 70-75%.
Which is not totally bad but taking a pre-trained network will do better.
Indeed, a pre-trained (on the imagenet dataset) MobileNetV2 reached an accuracy of around 90%. 
Misclassification between palmtree and winter wonderland images should be suppressed since its content clashes in general.
Closer inspection showed that a misclassification between those two classes basically never happened.
Misclassified images were mostly the ones were the object was only a minor part of the image.
For our purposes, this is fine.
The prediction of the neural network is written to the database.
</p>

</br></br><h3>3. Determine Aesthetics</h3>
   <p>Having the information on dominant colours and content category, we can now use these to automatically choose aesthetic image combinations.
An aesthetic is dominated by colour likeness. 
To quantify colour similarity, two images get a score relative to eachother.
This score is determined by the similarity threshold we choose. It corresponds to a certain distance in CIEDE2000-space.
To gauge the significance of a certain threshold, we note that the human eye's sensitivity is around 2-3. Colours closer than this can't be separated by perception.
For instance we can give a point for each colour closer than a similarity threshold 5 and another point if the colours are even closer than similarity threshold/2.</br></br>
Images can fit together colourwise but if their content clashes according to human perception,
it'll feel like something is off.
Hence we use the information on content to filter opposite content (for instance never allow palms and snow together).</p> 

 <h5>3.1 Nearest Neighbours</h5>
 <p>To get the aesthetic companion of an image, we just need to find the images with the highest score in regards to this image. </p> 

<h5>3.2 Cluster similar colours with K-Means</h5>
<p>
  Having a score for aesthetic, we can use this as input for the K-Means clustering algorithm and group images accordingly.
  Clustering is ideal for getting aesthetic pages in a photobook, the number of clusters could correspond to the number of pages.
We can get a sequence of images by continuing each cluster with the next nearest cluster. 
The image below shows five random clusters with a similarity threshold of 5 while ignoring content.

</p>
<img src="images/clusters1.jpg"  width=100% /></br></br>


<h3>Results</h3></br>

Some of the results are also shared on this <a href="https://www.instagram.com/gold.nuss/"> Instagram</a> profile while more details can be found on <a href="https://github.com/kokostino/Aesthetics-Automation"> Github</a>.
<hr>
</br></br><img src="images/seq1.jpg" alt="d"   class='center'/>
<p2>
</br></br><b>Information used</b>: Colour</br></br>
<b>Method</b>: KMeans clustering</br></br></br> 
<b>Details</b>: </br></br>
<ui>
<li>Assign a score for each image pair, depending on
how many of the main colours are closer than a certain threshold distance in colour space.</li></br>
<li>Take the score distribution of all images and cluster them via KMeans.</li></br>
<li>Sequences consist of images of a cluster and nearby clusters.</li>
</ui>

</p2>


</br></br><hr></br></br>
<img src="images/seq2.PNG" alt="d"  width=30% style="float:right"/>
<p2>

</br></br><b>Information used</b>: Content of class "snow" and colour</br></br>
<b>Method</b>: Filter by class, KMeans clustering</br></br></br> 
<b>Details</b>: </br></br>
<ui>
<li>Assign a score for each image pair.
High points for almost identical colours, medium points for similar colours and none if their distance in colour space is too large.
</li></br>
<li>Take the score distribution of all images and cluster them via KMeans.</li></br>
<li>Sequences consist of images of a cluster and nearby clusters.</li>
</ui></br></br>
An example result of this is shown on the right.
</p2>
<hr></br></br>
</div>
</body>
</html>